<!DOCTYPE html>
<!-- saved from url=(0037)https://jetd1.github.io/nerflets-web/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta name="description" content="Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision.">
  <meta name="keywords" content="nerflets, neural rendering, 3d scene representation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision</title>
 
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="lidar-nerf-files/css" rel="stylesheet">

  <link rel="stylesheet" href="lidar-nerf-files/bulma.min.css">
  <link rel="stylesheet" href="lidar-nerf-files/bulma-carousel.min.css">
  <link rel="stylesheet" href="lidar-nerf-files/bulma-slider.min.css">
  <link rel="stylesheet" href="lidar-nerf-files/fontawesome.all.min.css">
  <link rel="stylesheet" href="lidar-nerf-files/academicons.min.css">
  <link rel="stylesheet" href="lidar-nerf-files/index.css">

  <script src="lidar-nerf-files/jquery.min.js"></script>
  <script defer="" src="lidar-nerf-files/fontawesome.all.min.js"></script>
  <script src="lidar-nerf-files/bulma-carousel.min.js"></script>
  <script src="lidar-nerf-files/bulma-slider.min.js"></script>
  <script src="lidar-nerf-files/index.js"></script>
</head>
<body data-new-gr-c-s-check-loaded="14.1115.0" data-gr-ext-installed="">



<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">LiDAR-NeRF: Novel LiDAR View Synthesis via Neural Radiance Fields</h1>
          <!-- <p class="abstract"> Large-Scale scene reconstruction and neural rendering from direct inference. </p> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://jetd1.github.io/nerflets-web/">Xiaoshuai Zhang</a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a href="https://jetd1.github.io/nerflets-web/">Abhijit Kundu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://jetd1.github.io/nerflets-web/">Thomas Funkhouser</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://jetd1.github.io/nerflets-web/">Leonidas Guibas</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://jetd1.github.io/nerflets-web/">Hao Su</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://jetd1.github.io/nerflets-web/">Kyle Genova</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Google Research. &nbsp;&nbsp;</span>
            <span class="author-block"><sup>2</sup>Stanford University. &nbsp;&nbsp;</span>
            <span class="author-block"><sup>3</sup>University of California, San Diego. &nbsp;&nbsp;</span>
          </div>
          <!-- <h1 style="font-size:24px;font-weight:bold">CVPR 2021</h1> -->
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
  <!--             <span class="link-block">
                <a href="static/paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2303.03361" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- supp PDF Link. -->
<!--               <span class="link-block">
                <a href="static/supp.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supp</span>
                </a>
              </span> -->
              <!-- Video Link. 
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> 
              -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/jetd1/nerflets"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (soon)</span>
                  </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
    

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-bird0" align="center">
          <img src="lidar-nerf-files/teaser.png" width="75%" alt="overview_image">
          <div class="content has-text-justified">
          <p>We propose to represent the scene with a set of local neural radiance fields, named nerflets, which are trained with only 2D supervision. Our representation is not only useful for 2D tasks such as novel view synthesis and panoptic segmentation, but also capable of solving 3D-oriented tasks such as 3D segmentation and scene editing. The key idea is our learned structured decomposition (top right).</p>
          </div>
        </div>
      </div>
    </div>
  </div>
   	<div align="center">
		<!-- *Results above are generated using our single pretrained model without per-scene optimization -->
	</div>
</section>

    
<!--
<section class="hero teaser">
  <div class="hero-body">
    <div class="columns is-centered">
      <div class="column is-8">
        <video id="teaser" autoplay muted loop height="100%">
          <source src="https://storage.googleapis.com/nerfies-public/videos/teaser.mp4"
                  type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          <span class="dnerf">D-NeRF</span> turns selfie videos from your phone into free-viewpoint
          portraits (<i>nerfies</i>).
        </h2>
      </div>
    </div>
  </div>
</section>
-->


<section class="section">
  <div class="container">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We address efficient and structure-aware 3D scene representation from images. Nerflets are our key contribution -- a set of local neural radiance fields that together represent a scene. Each nerflet maintains its own spatial position, orientation, and extent, within which it contributes to panoptic, density, and radiance reconstructions. By leveraging only photometric and inferred panoptic image supervision, we can directly and jointly optimize the parameters of a set of nerflets so as to form a decomposed representation of the scene, where each object instance is represented by a group of nerflets. During experiments with indoor and outdoor environments, we find that nerflets: (1) fit and approximate the scene more efficiently than traditional global NeRFs, (2) allow the extraction of panoptic and photometric renderings from arbitrary views, and (3) enable tasks rare for NeRFs, such as 3D panoptic segmentation and interactive editing.
          </p>
          </div>
          <div align="center" style="margin-top:80px;">
            <img style="height: auto; width: 100%; object-fit: contain" src="lidar-nerf-files/pipeline.png" alt="overview_image">
            </div> 
            <p>
              Compared to vanilla NeRF, nerflets have good efficiency due to their local structure. Each nerflet is much smaller than a vanilla NeRF, and only the nerflets near a point sample need to be evaluated. As a result, we are able to build an interactive editing tool that edits and renders nerflets in real time. 
            </p>
      </div>
    </div>
  </div>
  <!--/ Abstract. -->
<!-- <section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 align="center"  class="title is-2">Results</h2>
        
      </div>
    </div>
    <!--/ Animation. -->
</section> --&gt;


<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 align="center" class="title is-2">Videos</h2>

            <!-- Main -->
            <!-- <h3 align="center" style="margin-top:80px;"  class="title is-4">Introduction Video</h3> -->
            <div class="content has-text-justified">
            </div>
            <div class="content has-text-centered">
              <video id="replay-video" controls="" muted="" width="80%">
                <source src="static/videos/nerflets_vid_v8.mp4" type="video/mp4">
              </video>
            </div>
            <!-- Main -->

      </div>
    </div>
    <!--/ Animation. -->
  </div>
</section>





<section class="section" id="BibTeX">
  <div class="container content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @article{zhang2022nerfusion,
        author    = {Zhang, Xiaoshuai and Kundu, Abhijit and Funkhouser, Thomas and Guibas, Leonidas and Su, Hao and Genova, Kyle},
        title     = {Nerflets: Local Radiance Fields for Efficient Structure-Aware 3D Scene Representation from 2D Supervision},
        journal   = {CVPR},
        year      = {2023},
      }
</code></pre>
  </div>
</section>
    

<footer class="footer">
  <div align="center" class="container">
    <div class="columns is-centered">
        <div class="content">
            This website is adopted from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
        </div>
      </div>
    </div>
</footer>



<div id="naptha_container0932014_0707"></div></body><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
  div.grammarly-desktop-integration {
    position: absolute;
    width: 1px;
    height: 1px;
    padding: 0;
    margin: -1px;
    overflow: hidden;
    clip: rect(0, 0, 0, 0);
    white-space: nowrap;
    border: 0;
    -moz-user-select: none;
    -webkit-user-select: none;
    -ms-user-select:none;
    user-select:none;
  }

  div.grammarly-desktop-integration:before {
    content: attr(data-content);
  }
</style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>