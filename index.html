<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">

  <meta name="description" content="LiDAR-NeRF: Novel LiDAR View Synthesis via Neural Radiance Fields">
  <meta name="keywords" content="lidar-nerf, neural rendering, 3d reconstruction, Novel LiDAR View Synthesis, Neural Implicit Fields, LiDAR-NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>LiDAR-NeRF: Novel LiDAR View Synthesis via Neural Radiance Fields</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/lidar_nerf_favicon_128.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="static/images/lidar_nerf_logo.png" width="40%" alt="overview_image">
          <h1 class="title is-1 publication-title">LiDAR-NeRF: Novel LiDAR View Synthesis via Neural Radiance Fields</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=1ltylFwAAAAJ&hl=zh-CN&oi=sra"><strong>Tao Tang</strong></a><sup>1</sup>,
              </span>
            <span class="author-block">
             <a href="https://damo.alibaba.com/labs/intelligent-transportation"><strong>Longfei Gao</strong></a><sup>2</sup>,
              </span>
            <span class="author-block">
              <a href="https://wanggrun.github.io/"><strong>Guangrun Wang</strong></a><sup>3</sup>,
              </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=2w9VSWIAAAAJ&hl=en"><strong>Yixing Lao</strong></a><sup>4</sup>,
              </span>
            <span class="author-block">
               <a href="https://damo.alibaba.com/labs/intelligent-transportation"><strong>Peng Chen</strong></a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://hszhao.github.io/"><strong>Hengshuang Zhao</strong></a></span><sup>4</sup>,
          </div>
          <div class="is-size-5 publication-authors">

            <span class="author-block">
              <a href="https://damo.alibaba.com/labs/intelligent-transportation"><strong>Dayang Hao</strong></a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=voxznZAAAAAJ"><strong>Xiaodan Liang</strong></a><sup>1*</sup>,
            </span>
            <span class="author-block">
             <a href="https://scholar.google.com/citations?user=n-B0jr4AAAAJ"><strong>Mathieu Salzmann</strong></a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com.hk/citations?user=Jtmq_m0AAAAJ&hl=zh-CN&oi=sra"><strong>Kaicheng Yu</strong></a><sup>2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup> Shenzhen Campus, Sun Yat-sen University. &nbsp;&nbsp;</span>
            <span class="author-block"><sup>2</sup>Autonomous Driving Lab, Alibaba Group. &nbsp;&nbsp;</span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>3</sup>University of Oxford. &nbsp;&nbsp;</span>
            <span class="author-block"><sup>4</sup>The University of Hong Kong. &nbsp;&nbsp;</span>
            <span class="author-block"><sup>5</sup> CVLab, EPFL. &nbsp;&nbsp;</span>
          </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2304.10406" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://youtu.be/YX4LX025mZQ"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/Trent-tangtao/lidar-nerf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>


            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
        <div class="item item-bird0" align="center">
          <img src="static/images/teaser.jpg" width="75%" alt="overview_image">
          <div class="content has-text-justified">
            <p>   <b>(Left)</b> We introduce the task of novel view synthesis for LiDAR sensors. Given multiple LiDAR viewpoints of an object, novel LiDAR view synthesis aims to render a point cloud of the object from an arbitrary new viewpoint.
              <b>(Right, Top)</b>  The mostly-closed related approaches to generating new LiDAR point clouds are some LiDAR simulators, which suffer from limited scalability and  applicability, and fails to produce realistic LiDAR patterns.
              Furthermore, traditional NeRFs are not directly applicable to point clouds.
                <b>(Right, Bottom)</b>  By contrast, we propose a novel differentiable framework, LiDAR-NeRF, with an associated neural radiance field, to avoid explicit 3D reconstruction and game engine usage. Our method enables end-to-end optimization and encompasses the 3D point attributes into the learnable field.

            </p>
          </div>

      </div>
    </div>
  </div>
  <div align="center">
  </div>
</section>



<section class="section">
  <div class="container">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce a new task, novel view synthesis for LiDAR sensors. While traditional model-based LiDAR simulators with style-transfer neural networks can be applied to render novel views, they fall short of producing accurate and realistic LiDAR patterns because the renderers rely on explicit 3D reconstruction and exploit game engines, that ignore important attributes of LiDAR points. We address this challenge by formulating, to the best of our knowledge, the first differentiable end-to-end LiDAR rendering framework, LiDAR-NeRF, leveraging a neural radiance field (NeRF) to facilitate the joint learning of geometry and the attributes of 3D points. However, simply employing NeRF cannot achieve satisfactory results, as it only focuses on learning individual pixels while ignoring local information, especially at low texture areas, resulting in poor geometry. To this end, we have taken steps to address this issue by introducing a structural regularization method to preserve local structural details. To evaluate the effectiveness of our approach, we establish an object-centric multi-view LiDAR dataset, dubbed NeRF-MVL. It contains observations of objects from 9 categories seen from 360-degree viewpoints captured with multiple LiDAR sensors. Our extensive experiments on the scene-level KITTI-360 dataset, and on our object-level NeRF-MVL show that our LiDAR-NeRF surpasses the model-based algorithms significantly.
          </p>
<!--        </div>-->
<!--        <div align="center" style="margin-top:80px;">-->
          <img style="height: auto; width: 100%; object-fit: contain" src="static/images/motivation3.jpg" alt="overview_image">
<!--        </div>-->
<!--        <div class="content has-text-justified">-->
        <p>
          A comparison of novel view LiDAR point clouds generated from LiDARsim, PCGen, and our LiDAR-NeRF. LiDARsim suffers from inaccuracies in explicit 3D mesh recon- struction. PCGen overestimates object surfaces. Specifically, laser beams emitted by the LiDAR sensor can be influenced by surface material and normal direction, resulting in some beams pene- trating car glass and reaching the seats (car1 and car2), while others are lost (car3). Although an additional style-transfer net can alleviate the problem of beam loss, it does not take into account spe- cial attributes like the transmission. As opposed to prior arts, our proposed method, LiDAR-NeRF, effectively encodes 3D information and multiple attributes, achieving high fidelity with ground truth.
        </p>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-2">Method</h2>
        <div class="content has-text-justified">
          <h2 class="title is-4">LiDAR-NeRF</h2>
          <img style="height: auto; width: 100%; object-fit: contain" src="static/images/lidar-nerf12.jpg" alt="overview_image">
          <p>
            <b>(a) (Top)</b> The physical model of a LiDAR can be described as follow: each laser beam originates from the sensor origin and shoot outwards to a point in the real world or vanishes. One common pattern of laser beams is spinning in a 360-degree fashion. <b>(Bottom)</b> We convert the point clouds into a range image, where each pixel corresponds to a laser beam. Note that we highlight one object in the different views to facilitate the visualization. <b>(b)</b> Taking multi-view LiDAR range im- ages with associated sensor poses as input, our model produces 3D representations of the distance, the intensity, and the ray-drop probability at each pseudo-pixel. We exploit multi-view consistency of the 3D scene to help our network produce accurate geometry. <b> (c)</b> The physical nature of LiDAR models results in point clouds exhibiting recognizable patterns, such as the ground primarily ap- pearing as continuous straight lines. This pattern is also evident in the transformed range images, which display significant structural features, such as their horizontal gradient being almost zero in flat areas. As a result, these structural characteristics are essential for the network to learn.
            </p>
          <h2 class="title is-4">LiDAR-MVL</h2>
          <img style="height: auto; width: 100%; object-fit: contain" src="static/images/path_dataset3.jpg" alt="overview_image">
          <p>
            <b>(a)</b> We design two square paths of collection, small and large with 7 and 15 meters in length respectively. <b> (b)</b> Our NeRF-MVL dataset encompasses 9 objects from common traffic categories. We align multiple frames here for better visualization.
            </p>
        </div>
      </div>
    </div>
  </div>

</section>



<section class="section">
  <div class="container">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-two-thirds">
        <h2 class="title is-2">Results</h2>
        <div class="content has-text-justified">
          <h2 class="title is-4">KITTI-360</h2>
          <img style="height: auto; width: 100%; object-fit: contain" src="static/images/kitti_res12.2.jpg" alt="overview_image">
          <p>
            Our LiDAR-NeRF produces more realistic LiDAR patterns with highly detailed structure and geometry.
          </p>
          <h2 class="title is-4">LiDAR-MVL</h2>
          <img style="height: auto; width: 100%; object-fit: contain" src="static/images/self_data_res8.jpg" alt="overview_image">
          <p>
            LiDAR-NeRF can effectively encode 3D information and multiple attributes, enabling it to accurately model the behavior of beams as they penetrate car glass and reach seats. Moreover, the high quality of the results obtained from different viewpoints serves as compelling evidence of our method’s effectiveness
          </p>
          <h2 class="title is-4">Ablations</h2>
          <img style="height: auto; width: 100%; object-fit: contain" src="static/images/reg5.jpg" alt="overview_image">
          <p>
            The iNGP’s hybrid 3D grid archi- tecture achieves more detailed structures. Our structural regularization significantly improves the geometry estimation and produces more realistic LiDAR patterns.
          </p>
          <h2 class="title is-4">Edit</h2>
          <img style="height: auto; width: 100%; object-fit: contain" src="static/images/edit_paste11.jpg" alt="overview_image">
          <p>
            The augmented scene from our LiDAR-NeRF has realistic occlusion effects and consistent LiDAR pattern thanks to our pseudo range-image formalism, compared with the common cope-paste strategy.
          </p>
        </div>
      </div>
    </div>
  </div>

</section>


<section class="section">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 align="center" class="title is-2">Videos</h2>

        <!-- Main -->
        <!-- <h3 align="center" style="margin-top:80px;"  class="title is-4">Introduction Video</h3> -->
        <div class="content has-text-justified">
        </div>
        <div class="content has-text-centered">
          <video id="replay-video" controls="" muted="" width="80%">
            <source src="static/video/lidar_nerf_video.mp4" type="video/mp4">
          </video>
        </div>
        <!-- Main -->

      </div>
    </div>
    <!--/ Animation. -->
  </div>
</section>





<section class="section" id="BibTeX">
  <div class="container content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
 @article{tao2023lidar,
  title={LiDAR-NeRF: Novel LiDAR View Synthesis via Neural Radiance Fields},
  author={Tao, Tang and Gao, Longfei and Wang, Guangrun and Lao, Yixing and Chen, Peng and Zhao hengshuang and Hao, Dayang and Liang, Xiaodan and Salzmann, Mathieu and Yu, Kaicheng},
  journal={arXiv preprint arXiv:2304.10406},
  year={2023}
}
</code></pre>
  </div>
</section>


<footer class="footer">
  <div align="center" class="container">
    <div class="columns is-centered">
      <div class="content">
        This website is adopted from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a>.
      </div>
    </div>
  </div>
</footer>



</body>
</html>
